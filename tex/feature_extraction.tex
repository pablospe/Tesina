\section{Feature extraction}
\label{feature_extraction}

Las técnicas usuales para \textit{handwriting recognition} tratan de encontrar \textit{features} particulares sobre un conjunto de símbolos, citemos por ejemplo los dígitos. Pero al cambiar dicho conjunto, estos features dejan de ser efectivos, no discriminan correctamente. Se vuelve impráctico desarrollar heurísticas para reconocer features específicos para cada símbolo.
% Sobre todo si se tiene en cuenta que símbolos matemáticos pueden ser inventados o agregados en la marcha.
Por lo que es deseable buscar una representación que permita ser aplicada sin importar qué tipo de símbolo se trate; ya sea un dígito, una letra o un símbolo matemático.

Unos de los mayores problemas con los métodos de reconocimiento tradicionales es que los trazos son tratados como secuencias de puntos (en \textit{discreto}), en vez de verlos como lo que realmente son, curvas (en \textit{continuo}).
% son pensados como una secuencia de puntos. El problema con ésto es que no se lo están tratando como lo que realmente son, curvas.

\subsection{Trazos discretos como curvas continuas}
\label{Trazos_como_curvas_continuas}

En vez de describir a los trazos como una secuencia de puntos, éstos pueden ser representados por aproximaciones de curvas, figura~\ref{fig:trazos_vs_curvas}. Se mostrará que se necesitan menos de vein\-te~($20$) coeficientes de una serie para representar un trazo.
% \vspace*{-0.1cm}
    \begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.4]{imagen/trazos_como_curva.pdf}
%     \includegraphics[scale=0.23]{imagen/x_e_y_en_t.pdf}
    \caption{Trazos como curvas paramétricas: $r(t) = \{x(t),y(t)\}$ }
    \label{fig:trazos_vs_curvas}
    \end{figure}

\vspace*{-0.2cm}
\noindent
Observar que se necesitan aproximar dos curvas: $x(t)$, $y(t)$ por símbolo, figura~\ref{fig:x_e_y_en_t}.
% \vspace*{-0.15cm}
    \begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.29]{imagen/x_e_y_en_t.pdf}
    \caption{En \textcolor{red}{rojo} las aproximaciones de $x(t)$, $y(t)$}
    \label{fig:x_e_y_en_t}
    \end{figure}

\vspace*{-0.2cm}
\noindent
Trabajos anteriores \cite{Succinct} han demostrado cómo las coordenadas $x(t)$, $y(t)$ de símbolos escritos a mano pueden ser representados como expansión en series de polinomios ortogonales de \textit{Chebyshev}; y que los coeficientes de las series truncadas además de ser las aproximaciones buscadas, pueden ser usados para clasificación y reconocimiento.

De manera similar, en este trabajo se utiliza un número finito de \textit{\textbf{momentos matemáticos}} para la reconstrucción de funciones (las curvas) como series truncadas de polinomios ortogonales de \textit{\textbf{Legendre}}, siguiendo las ideas presentadas en los artículos \cite{Moments} y \cite{OnlineModeling}. Este problema es conocido como \textit{\textbf{Hausdorff Moment Problem}} \cite{HausdorffMomentProblem}. Una alternativa a este método para la obtención de las aproximaciones a las curvas (es decir, los coeficientes de las series truncadas) es utilizar \textit{aproximación por \textbf{mínimos cuadrados}} a través de la \textit{\textbf{pseudo-inversa} de Moore-Penrose}, permitiendo este enfoque utilizar polinomios ortogonales cualesquiera; en particular, polinomios de \textit{\textbf{Legendre}}, \textit{\textbf{Legendre-Sobolev}} y \textit{\textbf{Chebyshev}}.

% usar diferentes tipos de polinomios ortogonales, en particular \textit{\textbf{Legendre}}, \textit{\textbf{Legendre-Sobolev}} y \textit{\textbf{Chebyshev}}.


% En lo siguiente se irán introduciendo algunos conceptos para finalmente llegar a la explicación de qué son los features, y cómo obtenerlos.


\subsection{Representación con series}
\label{sec:Representacion_con_series}
Se desea entonces aproximar las funciones $x(t), y(t)$ de los trazos. Como se indicó en la sección~\ref{Series_de_funciones_ortogonales}, ésto puede hacerse como expansión de series de polinomios ortogonales; según la ecuación~(\ref{eq:alpha_inner_prod}),
\begin{equation}
\label{eq:features}
\left\{
\begin{array}{rcl}
% \begin{equation}
% \begin{split}
x(t) & \approx & \sum_{i=0}^{d}{\alpha}_{i}\,{B}_{i}(t) \\
y(t) & \approx & \sum_{i=0}^{d}{\beta}_{i}\,{B}_{i}(t)
% \end{split}
% \end{equation}
\end{array}
\right.
\end{equation}

% \noindent
La clasificación puede ser obtenida, por ejemplo, al medir la distancia Euclidiana de los vectores $({\alpha}_{0}, \dots ,{\alpha}_{d}, {\beta}_{0}, \dots ,{\beta}_{d})$ entre los distintos trazos.

Consideraremos \textbf{features} a estos vectores que tienen dimensión $2\:(d+1)$. En lo siguiente, se mostrarán dos formas de calcularlos y, en secciones posteriores, resultados que avalen que los features son lo suficientemente representativos como para clasificar de manera precisa a los trazos.

% Se mostrará dos formas de calcularlos, una por medio de momentos matemáticos (sección \ref{HausdorffMomentProblem}) y otra a través aproximación por mínimo cuadrados usando la pseudo-inversa (sección \ref{leastSquare}), y resultados que avalen que los features son lo suficientemente representativos como para clasificar de manera precisa los trazos.




\subsection{Momentos}
\label{Momentos}

Una posible forma de calcular los $\{\alpha_i\}$ y $\{\beta_i\}$ es mediante momentos, como se explicará a continuación.

\subsubsection{Definición}
Los momentos matemáticos de una función $f$ definida en $[0,1]$ son
\begin{align}
\label{eq:momentos}
\mu_{k} & \doteq \int_{0}^{1}{f(\lambda)\,\lambda^k\,d\lambda}
\end{align}


\subsubsection{Hausdorff Moment Problem}
\label{HausdorffMomentProblem}
Este problema consiste en recuperar a partir de una secuencia finita de  momentos $\{\mu_k\}_{k=0,1,2,...}$ una función $f$ en el dominio $[0,1]$.
% Para un desarrollo más teórico del problema ver \cite{Moments}.
% Aquí se presenta una explicación simplificada.

Con el fin de facilitar el desarrollo, recordaremos algunas definiciones y ecuaciones previas.
%de la reconstrucción de funciones mediante momentos.
Se supondrá que el dominio es $[0,1]$, y que la función peso es $w(t)=1$ para poder trabajar con los polinomios de \textit{Legendre} (shifted), sección \ref{legendre}. Por lo que el producto interno en~\refEQ{def:inner_product} queda
\begin{align}
\label{eq:inner_product_2}
 \langle f, g \rangle & = \int_{0}^{1}f(t)\,g(t)\,dt
\end{align}
Escribamos nuevamente la ecuación \refEQ{eq:legendre},
\begin{align*}
L_i(t) &= \sum_{j=0}^{i}{ C_{ij}\,t^j }
\end{align*}
y también la ecuación \refEQ{eq:aprox}, pero con el reemplazo de ${B}_{i}$ por ${L}_{i}$,
\begin{align*}
f(t) & \approx \sum_{i=0}^{d}{\alpha}_{i}\,{L}_{i}(t)
\end{align*}
Ahora, a partir de \vspace*{-0.4cm}
% \refEQ{eq:alpha_inner_prod}
% \begin{align*}
% {\alpha}_{i} & = \langle f, L_i \rangle
% \end{align*}
% & =  \quad \langle \rangle  \\
% & =  \quad \langle \rangle  \\
% & =  \quad \langle \rangle  \\
% & =  \quad \langle \rangle  \\
\begin{align*}
\hspace*{4cm}  & {\alpha}_{i}  \\
 = \quad &   \quad \langle \text{~ por}~\refEQ{eq:alpha_inner_prod} ~ \rangle  \\
   &   \langle f, L_i \rangle \\
 = \quad &   \quad \langle \text{~ expansión del producto interno}~\refEQ{eq:inner_product_2} ~ \rangle  \\
   &   \int_{0}^{1}f(t)\,L_i(t)\,dt \\
 = \quad &   \quad \langle \text{~ definición de $L_i(t)$, ecuación}~\refEQ{eq:legendre} ~ \rangle  \\
   &   \int_{0}^{1}f(t)\, \left( \sum_{j=0}^{i}{ C_{ij}\,t^j } \right) \,dt \\
 = \quad &   \quad \langle \text{~ reordenación de términos, sumatoria afuera} ~ \rangle  \\
   &   \sum_{j=0}^{i}{ C_{ij}\,\underbrace{\left( \int_{0}^{1}f(t)\,t^j\,dt \right)}_{\mu_j} } \\
 = \quad &   \quad \langle \text{~ definición de momento}~\refEQ{eq:momentos} ~ \rangle  \\
   &   \sum_{j=0}^{i}{ C_{ij}\,\mu_j}
\end{align*}


\noindent
Se puede escribir en forma matricial
\begin{align*}
% alpha vector
   \begin{bmatrix}
    \alpha_{0} \\
    \alpha_{1} \\
    \vdots \\
    \alpha_{d}
   \end{bmatrix}_{(d+1)}
& =
% C matrix
    \begin{bmatrix}
      C_{00} \\
      C_{10} & C_{11} \\
      \vdots & \vdots & \ddots \\
      C_{d0} & C_{d1} & \dots & C_{dd}
    \end{bmatrix}_{(d+1)\times(d+1)} \dot ~
% mu vector
\begin{bmatrix}
    \mu_{0} \\
    \mu_{1} \\
    \vdots \\
    \mu_{d}
   \end{bmatrix}_{(d+1)}
\end{align*}
Resumiendo,
\begin{align}
\label{eq:HMP}
   \mathbf{\alpha} & = \mathbf{C\,\mu}
\end{align}
donde $\mathbf{C}$ es la matriz de coeficientes de los polinomios de Legendre, y $\mathbf{\mu}$ un vector columna con los momentos matemáticos de la función a reconstruir. De esta manera, se tiene que el cálculo de~$\mathbf{\alpha}$, los coeficientes de la expansión finita por series de la función~$f$, se realiza a partir de una multiplicación matricial en la que intervienen los momentos.


% \subsection{Cálculo de los momentos}
Observar que en la ecuación anterior~\refEQ{eq:HMP}, la matriz~$\mathbf{C}$ puede ser \textbf{precalculada} con la fórmula cerrada~\refEQ{eq:legendre_coefficients}, o mediante el proceso de Gram-Schmidt~\refEQ{eq:Gram-Schmidt}. Cualquiera sea la forma en que se calculen los coeficientes de los polinomios de Legendre, es importante notar que solo es necesario realizarlo por única vez, por lo que es necesario focalizarse en el cálculo de los momentos.


\subsubsection{Momentos para funciones definidas en otros dominios}
Generalicemos la definición \refEQ{eq:momentos}, sea $f$ una función definida en $[0,\ell]$ los momentos de~$f$ son
\begin{align}
\label{eq:momentos_con_param}
\mu_{k}(f,\ell) \, & =\int_{0}^{\ell}{f(\lambda)\,\lambda^k\,d\lambda} \hspace*{13.5ex}
\end{align}
%  $[0,\ell]$
% sección \ref{legendre}

Al estar los polinomios de Legendre (shifted) definidos en $[0,1]$ es necesario calcular los momentos en ese dominio. Entonces, el problema aquí es cómo calcularlos a partir de una función~$f$ definida en $[0,\ell]$. Esto se resuelve con un \textbf{cambio de variable} como se verá de inmediato.

Obsérvese que si se define,
\begin{align}
\gamma  & \doteq \frac{1}{\ell}\,\lambda  \quad \Longrightarrow \quad \lambda \in [0,\ell] \Leftrightarrow \gamma \in [0,1]  \hspace*{7ex}
\end{align}
también definamos,
\begin{align}
\label{eq:f_hat}
\hat{f}(\gamma) & \doteq f(\ell\,\gamma)  \hspace*{18ex} % = f(\lambda)
\end{align}

Ahora, partiendo de la definición \refEQ{eq:momentos_con_param},
\begin{align*}
\hspace*{4cm}  \mu_{k}(f,\ell) & = \int_{0}^{\ell}{f(\lambda)\,\lambda^k\,d\lambda}, \hspace*{7.9ex}
\begin{footnotesize}
\text{\textbf{cambio de variable}:}~
\left\{\hspace*{-1ex}
            \begin{array}{rl}
               \lambda  = & \hspace*{-2ex} \ell\,\gamma \\
               d\lambda = & \hspace*{-2ex} \ell\,d\gamma
            \end{array}
\right.
\end{footnotesize} \\
& = \int_{0}^{1}{f(\ell\,\gamma)\,(\ell\,\gamma)^k\,\ell\,d\gamma}, \hspace*{2.4ex}
\begin{footnotesize}\text{reordenando}\end{footnotesize} \\
& = \ell^{k+1}\int_{0}^{1}{f(\ell\,\gamma)\,\gamma^k\,d\gamma}, \quad
\begin{footnotesize}\text{por \refEQ{eq:f_hat}}\end{footnotesize} \\
& = \ell^{k+1}\underbrace{\int_{0}^{1}{\hat{f}(\gamma)\,\gamma^k\,d\gamma}}_{\mu_{k}(\hat{f},1)}, \hspace*{3.3ex}
\begin{footnotesize}\text{por \refEQ{eq:momentos_con_param}}\end{footnotesize} \\
& = \ell^{k+1}\,\mu_{k}(\hat{f},1)
\end{align*}
obtenemos,
\begin{align}
\label{eq:momentos_f_hat}
\mu_{k}(\hat{f},1)  & = \frac{\mu_{k}(f,\ell)}{\ell^{k+1}} \hspace*{19ex}
\end{align}


Por lo tanto, se utilizarán los momentos generalizados de la función~$f$ original en $[0,\ell]$ para calcular los momentos de la función~$\hat{f}$ en el dominio que se necesitan, es decir, $[0,1]$.

\subsubsection{Invariante a cambios de escala}
\label{Invariante_escala}
Recordemos la ecuación \refEQ{eq:HMP},
\begin{align*}
   \mathbf{\alpha} & = \mathbf{C\,\mu}
\end{align*}
Se demostrará que la normalización de este vector, es decir~$\frac{\alpha}{\|\alpha\|}$, es invariante a escala. Supongamos que tenemos una función~$f$ y la multiplicamos por un factor positivo~$s$, a la función resultante la llamaremos~$\hat{f}$, es decir,
\begin{align*}
\hat{f}(t) = s\,f(t), \quad\quad s>0
\end{align*}
Veamos cómo esto afecta al cálculo de momentos,
\begin{align*}
\mu_{k}(\hat{f},\ell) & = \int_{0}^{\ell}{\hat{f}(\lambda)\,\lambda^k\,d\lambda} \\
                   & = \int_{0}^{\ell}{s\,f(\lambda)\,\lambda^k\,d\lambda} \\
                   & = s\,\mu_{k}(f,\ell)
\end{align*}
Utilizando \refEQ{eq:HMP} para el cálculo de $\hat{\alpha}$, correspondiente a la función $\hat{f}$
\begin{align*}
   \hat{\alpha} & = \mathbf{C\,\hat{\mu}} = \mathbf{C\,(s\,\mu)} = s\,\mathbf{C\,\mu} = s\,\alpha
\end{align*}
Recordando que $s>0$, se obtiene
\begin{align*}
\frac{\hat{\alpha}}{\|\hat{\alpha}\|} & =
    \frac{s\,\alpha}{\|s\,\alpha\|} =
    \frac{s\,\alpha}{|s|\,\|\alpha\|} =
    \frac{\alpha}{\|\alpha\|}
\end{align*}
Resumiendo,
\begin{align}
\label{eq:invaciancia_escala}
\frac{\hat{\alpha}}{\|\hat{\alpha}\|} & = \frac{\alpha}{\|\alpha\|}
\end{align}
Se puede concluir entonces que al normalizar los vectores a norma $1$, los \textbf{features son invariante a escala}.


\subsubsection{Cálculo numérico de los momentos}
\label{sec:calculo_numerico_momentos}
Se asume que muestras de $f$ se irán recibiendo en tiempo real a medida que el usuario realiza el trazo. En principio, se podría esperar a que el trazo sea finalizado para comenzar el cálculo de los momentos, pero se mostrará que es posible ir calculándolos a medida que los datos van llegando. De esta manera se logra que la extracción de features sea extremadamente eficiente, pues una vez que el usuario levanta su lápiz (finalización del trazo), los momentos matemáticos ya han sido calculados. Pero antes, nos enfocaremos en el cálculo numérico.

Obsérvese que la integral siguiente debe aproximarse
\begin{align*}
\mu_{k}(f,\ell) & = \int_{0}^{\ell}{f(\lambda)\,\lambda^k\,d\lambda}
\end{align*}

Se ha intentado con las reglas del \textit{\textbf{Trapecio}} (primer orden) y \textit{\textbf{Simpson}} (segundo orden), pero no se ha alcanzado suficiente precisión para los valores de $\mu_k$ cuando $k$ crece. Métodos que requieran el conocimiento de $\ell$ tampoco pueden ser aplicados, por lo explicado en el primer párrafo.

Un método adecuado es una instancia especializada del método de integración de primer orden, debido a \cite{OnlineModeling}. Para computar $\int_{0}^{\ell}{f(\lambda)\,\lambda^k\,d\lambda}$ se usa la siguiente aproximación en cada intervalo $[i,i+1]$, %se asume que los puntos están igualmente espaciados con $\Delta\lambda=1$,
\begin{align}
\int_{i}^{i+1}{f(\lambda)\,\lambda^k\,d\lambda}
  & \approx \underbrace{\frac{f(i+1)+f(i)}{2}}_{\text{valor medio}} * \underbrace{\int_{i}^{i+1}{\lambda^k\,d\lambda}}_{\text{integral exacta}} \nonumber \\
  & = \frac{f(i+1)+f(i)}{2} * \left.\frac{\lambda^{k+1}}{k+1}\right|_{i}^{i+1} \nonumber \\
  \label{eq:integracion}
  & = \frac{f(i+1)+f(i)}{2} * \frac{(i+1)^{k+1} - (i)^{k}}{k+1}
\end{align}
Esta fórmula representa la integral exacta en $\lambda^k$ en $[i,i+1]$ multiplicado por el valor medio de la función entre esos puntos. Sumando los intervalos,
\begin{align}
\mu_{k}(f,\ell)
  & = \int_{0}^{\ell}{f(\lambda)\,\lambda^k\,d\lambda} \nonumber \\
  & = \sum_{i=0}^{\ell-1}{ \int_{i}^{i+1}{f(\lambda)\,\lambda^k\,d\lambda} }, \hspace*{1cm}  \text{aplicando}~\refEQ{eq:integracion}~\text{se obtiene} \nonumber \\
  \label{eq:integracion_total}
  & \approx \sum_{i=0}^{\ell-1}{ \frac{f(i+1)+f(i)}{2} * \frac{(i+1)^{k+1} - (i)^{k}}{k+1} } \\
  \label{eq:integracion_total_2}
  & = \sum_{i=1}^{\ell-1}{ \frac{f(i-1)+f(i+1)}{2} * \frac{i^{k+1}}{k+1} } \quad + \quad \frac{f(\ell-1)+f(\ell)}{2} * \frac{\ell^{k+1}}{k+1}
\end{align}
La última igualdad puede obtenerse expandiendo los términos de la ecuación \refEQ{eq:integracion_total}, sucede una situación similar a las series telescópicas en las cuales se anulan los términos intermedios, solo quedando los extremos. La cantidad de términos a calcular se reduce a la mitad, hecho importante en la búsqueda de mayor eficiencia computacional.

En la ecuación \refEQ{eq:integracion_total_2}, se puede ir actualizando la suma $\sum_{i=1}^{\ell-1}{(\dots)}$ para todos los \textit{órdenes} $k$ de $0$ a $d$ tan pronto como el valor de $f(i+1)$ esté disponible, permitiendo así \textbf{calcular los momentos a medida que se ingresa el trazo}.
\subsection{Pseudo-inversa de Moore-Penrose}
\label{leastSquare}
Otra forma de calcular los $\alpha$ de la ecuación~\refEQ{eq:aprox} es a través de la pseudo-inversa de Moore-Penrose. Esta solución no solo funciona con los polinomios de Legendre sino también para polinomios ortogonales $\{L_i\}$ cualesquiera. Definamos la siguiente función:
\begin{align*}
p(t) & = \sum_{i=0}^{d}{\alpha}_{i}\,{L}_{i}(t) \\
     & = \alpha_{0}\,{L}_{0}(t) + \alpha_{1}\,{L}_{1}(t) + \dots + \alpha_{d}\,{L}_{d}(t)
\end{align*}

Planteamos el siguiente sistema sobre-determinado de ecuaciones
\begin{equation*}
\left\{
    \begin{array}{rl}
p(x_0) & = \alpha_{0}\,{L}_{0}(x_0) + \alpha_{1}\,{L}_{1}(x_0) + \dots + \alpha_{d}\,{L}_{d}(x_0) \\
p(x_1) & = \alpha_{0}\,{L}_{0}(x_1) + \alpha_{1}\,{L}_{1}(x_1) + \dots + \alpha_{d}\,{L}_{d}(x_1) \\
\vdots & \\
p(x_d) &= \alpha_{0}\,{L}_{0}(x_d) + \alpha_{1}\,{L}_{1}(x_d) + \dots + \alpha_{d}\,{L}_{d}(x_d)
    \end{array}
\right.
\end{equation*}
que puede escribirse matricialmente como
\begin{align*}
% p vector
\underbrace{
    \begin{bmatrix}
        p_{0} \\
        p_{1} \\
        \vdots \\
        p_{d}
    \end{bmatrix}%_{(d+1)}
}_{P}
& =
% L matrix
\underbrace{
    \begin{bmatrix}
      L_{0}(x_0) & L_{1}(x_0) & \dots & L_{d}(x_0) \\
      L_{0}(x_1) & L_{1}(x_1) & \dots & L_{d}(x_1) \\
      \vdots     & \vdots     & & \vdots \\
      L_{0}(x_d) & L_{1}(x_d) & \dots & L_{d}(x_d) \\
    \end{bmatrix}%_{(d+1)\times(d+1)}
}_{L}
\dot ~
% alpha vector
\underbrace{
   \begin{bmatrix}
    \alpha_{0} \\
    \alpha_{1} \\
    \vdots \\
    \alpha_{d}
   \end{bmatrix}%_{(d+1)}
}_{\alpha} \\
P &= L\,\alpha
\end{align*}

Es sabido que la solución del siguiente problema de minimización
\begin{align*}
\alpha & = \underset{\alpha}{\operatorname{arg\,mix}} ~~ \|P-L\alpha\|^{2},
\end{align*}
es la estima de mínimos cuadrados dada por
\begin{align*}
\hat{\alpha} & = (L^{T}L)^{-1}L\,P \\
             & = L^{\dag}\,P
\end{align*}
donde $L^{\dag}$ es la pseudo-inversa de Moore-Penrose.

\subsection{Parametrización por longitud de arco}
\label{sec:arc-length}
Parametrización por tiempo y por longitud de arco (\textit{arc-length}) son las elecciones más populares en el campo de reconocimiento de escritura online, ver \cite{Nalwa} y \cite{Toward_affine_recognition}. Parametrización por longitud de arco es usualmente preferible, pues no es afectada por variaciones en la velocidad de escritura. Esta puede ser expresada como:
\begin{align*}
    \text{\textit{arc-length}}(\lambda) = \int_{0}^{\lambda}{\sqrt{(x'(\lambda))^2+(y'(\lambda))^2}}
\end{align*}


\subsection{Necesidad del preproceso}
Se ha demostrado que el método es invariante a escala \ref{Invariante_escala}, y comentado en \ref{sec:arc-length} que la parametrización por longitud de archo no es afectada por las variaciones en la velocidad de escritura. Pero no se ha mencionado que sea invariante a traslación, pues no lo es. Sí se puede probar empíricamente que a pesar de que no sea invariante a traslación, pequeñas traslaciones no perturban demasiado el cálculo de los vectores, obteniendo igualmente resultados razonables que pueden ser usados en otros escenarios en donde la eficiencia computacional es un requerimiento determinante, como ser los dispositivos móviles. De aquí se deriva el análisis de si es posible evitar el preproceso, el cual se posterga hasta la sección \ref{prepoceso}.