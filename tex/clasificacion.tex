\section{Clasificación}
\label{clasificacion}

En esta sección se introducirá los métodos de clasificación usados. Recordar que nuestros \textit{\textbf{features}} son vectores de dimensión $2\,(d+1)$, dados por la ecuación \refEQ{eq:features} de la sección \ref{sec:Representacion_con_series}, es decir, tienen la siguiente forma:
\begin{align*}
({\alpha}_{0}, \dots ,{\alpha}_{d}, {\beta}_{0}, \dots ,{\beta}_{d})
\end{align*}

% Los métodos de clasificación utilizados son: vecino más cercano y SVM, que se describen a continuación.

\subsection{Vecinos más cercanos: $k$-NN}
Este método intenta encontrar los $k$ vecinos más ``cercanos'' de un vector feature de entrada con respecto a los vectores features de una base de datos. El significado de ``cercano'' es asociado a una métrica. Las distancias que se usaron fueron: distancia \textit{\textbf{Euclidiana}}, de \textit{\textbf{Hamming}}\footnote{Existe dos versiones de esta distancia: una, a nivel de bit; y otra, a nivel de componentes de vectores. Se utilizó la última, también conocida como \textit{\textbf{cityblock}}} y de \textit{\textbf{Mahalanobis}}. En este trabajo solo se considera el \textit{\textbf{primer match}}, es decir, $k=1$.

Las distancias se especifican definiendo el operador $dist(x,y)$, donde $x = (x_0,x_1, \dots, x_d)$~e $y = (y_0,y_1, \dots, y_d)$ son vectores de dimensión $d+1$. Veamos a continuación las distancias utilizas en este trabajo.

\subsubsection{Distancia Euclidiana}
\begin{align}
\begin{split}
dist(x,y)^2 & = \sum_{i=0}^d (x_i-y_i)^2 \\
         & = (x-y)\,(x-y)^T
\end{split}
\end{align}

\subsubsection{Distancia de Hamming o Cityblock}
\begin{align}
dist(x,y) & = \sum_{i=0}^d |x_i-y_i|
\end{align}

\subsubsection{Distancia Mahalanobis}
Esta medida se diferencia de la distancia euclídea en que se tiene en cuenta la correlación entre las variables aleatorias,
\begin{align}
dist(x,y)^2 & = (x-y)\,C^{-1}\,(x-y)^T
\end{align}
donde $C$ es la matriz de covariancia.





\subsection{Support Vector Machine}

Las Máquinas de Vectores de Soporte (\textit{\textbf{Support Vector Machines}}, o \textit{\textbf{SVMs}}) es una reciente técnica de aprendizaje supervisado desarrollado por \cite{SVM, SVM2}. SVM ha sido aplicado exitosamente a un gran número de problemas de clasificación, y es adecuado para nuestro propósito.

Como ejemplo introductorio, suponga un problema de clasificación separable en un espacio de dos dimensiones. Hay muchos hiperplanos que pueden separar las dos clases de datos, ver figura~\ref{fig:svm_1}-(a).
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.9]{imagen/svm_1.pdf}
    \caption{(a) Posibles hiperplanos; (b) selección de un único hiperplano que maximiza la distancia de los puntos más cercanos de cada clase; (c) hiperplano de separación óptimo que maximiza el margen.}
    \label{fig:svm_1}
\end{figure}

Suponga un hiperplano tal que los puntos $x$ que se encuentren en el hiperplano satisfacen
\begin{align}
\label{eq:hiperplano}
\omega^Tx+b &= 0,
\end{align}
donde $\omega$ es la normal del hiperplano, $\frac{|b|}{\|\omega\|}$ es la distancia perpendicular desde el hiperplano al origen, y $\|\omega\|$ es la norma Euclidiana de $\omega$.

Definiendo \textit{\textbf{margen}} ($M$) como la suma de las distancias entre este hiperplano y los puntos más cercanos de cada clase, figura~\ref{fig:svm_1}-(b); el hiperplano de separación será óptimo si maximiza dicho margen, figura~\ref{fig:svm_1}-(c).


\subsubsection{SVM lineal}
\label{svm_lineal}
Si tenemos un conjunto de entrenamiento $\mathcal{D}$ de la forma
\begin{align*}
\mathcal{D} & = \left\{ (\mathbf{x}_i, y_i)\mid\mathbf{x}_i \in \mathbb{R}^p,\, y_i \in \{-1,1\}\right\}_{i=1}^n,
\end{align*}
donde $y_i$ puede ser $-1$ o $1$, indicando la clase a la cual pertenece el punto $x_i$. Se desea encontrar en~\refEQ{eq:hiperplano} los $w$ y $b$ que maximicen el margen, sujeto a las siguientes restricciones,
\begin{align*}
\omega^Tx_k-b &\geq~ ~ ~1, \hspace*{1.5cm} \mbox{for} \hspace{1mm} y_k=~ ~ ~1\\
\omega^Tx_k-b &\leq-1, \hspace*{1.5cm} \mbox{for} \hspace{1mm} y_k=-1
\end{align*}
Estas ecuaciones pueden combinarse con el siguiente conjunto de inecuaciones,
\begin{align}
\label{eq:svm}
y_k(\omega^Tx_k-b)-1\geq0, \hspace{3mm} k=1,...,n.
\end{align}

Notar que si $\mathcal{D}$ es linealmente separable, se pueden seleccionar dos hiperplanos de tal manera que no haya puntos entre ellos, maximizando sus distancias, ver figura \ref{fig:Svm_max_sep_hyperplane_with_margin}. Estos hiperplanos pueden describirse como: $\omega^Tx_k-b=1$ o $\omega^Tx_k-b=-1$, según la clase a la que $x_k$ corresponda. Y las distancias al origen de los hiperplanos son: $\frac{|b-1|}{\|\omega\|}$ y $\frac{|b+1|}{\|\omega\|}$, respectivamente. Entonces, el margen $M$ es igual a $\frac{2}{\|\omega\|}$ y el problema es resulto minimizando $\|w\|$ sujeto a \refEQ{eq:svm}. Por conveniencia matemática, se minimiza $\frac{1}{2}\,\|w\|^2$ sin cambiar la solución pues el mínimo es el mismo.
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.25]{imagen/Svm_max_sep_hyperplane_with_margin.png}
    \caption{Márgenes para un SVM entrenado con datos de dos clases. Las vectores en los márgenes son llamados \textit{\textbf{vectores soportes}}}
    \label{fig:Svm_max_sep_hyperplane_with_margin}
\end{figure}


Idealmente, el modelo basado en SVM debería producir un hiperplano que separe completamente los datos en dos clases. Sin embargo, una separación perfecta no siempre es posible, ver figura \ref{fig:nonsep}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.8]{imagen/svm_2.pdf}
    \caption{Problema de clasificación no separable}
    \label{fig:nonsep}
\end{figure}

En estos casos, son añadidas $\xi_k$ (\textit{variables slack}) en la formulación del problema con el objetivo de relajar las restricciones. Luego, el conjunto de inecuaciones toman la siguiente forma,
\begin{align*}
% \label{eq:svm3}
y_k(\omega^Tx_k-b)\geq1-\xi_k, \hspace{3mm} k=1,...,n .
\end{align*}


La función objetivo es entonces incrementada por una función que penaliza los $\xi_k$. La optimización se vuelve un balance entre tener el margen y la penalización de errores,
\begin{eqnarray*}
\min_{\omega,b,\xi}  &  %J_P(\omega,\xi) =
\hspace*{-3ex}  \frac{1}{2} \|w\|^2 + C \sum_{k=1}^n \xi_k \\
\mbox{s.t.}   & \hspace*{1ex} y_k(\omega^Tx_k-b)  \geq  1-\xi_k, & k=1,...,n \\
  & \hspace*{7ex} \xi_k  \geq  0, &  k=1,...,n.
\end{eqnarray*}
donce $C$ es el que controla la compensación entre errores de entrenamiento y los márgenes rígidos, creando así un margen blando (\textit{soft margin}) que permita algunos errores en la clasificación a la vez que los penaliza.

\subsubsection{SVM no lineal - Kernels}
El algoritmo resultante es esencialmente el mismo, excepto que todo producto escalar es reemplazado por una función kernel no lineal, ver figura \ref{fig:Kernel_Machine}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.5]{imagen/Kernel_Machine.png}
    \caption{Kernel: $K(x_k,x_\ell)=\phi(x_k)^T\phi(x_\ell)$}
    \label{fig:Kernel_Machine}
\end{figure}

Finalmente, el problema a optimizar es
\begin{eqnarray*}
\min_{\omega,b,\xi}  &  %J_P(\omega,\xi) =
\hspace*{-4ex}  \frac{1}{2} \|w\|^2 + C \sum_{k=1}^n \xi_k \\
\mbox{s.t.}   & \hspace*{2ex} y_k(\omega^T\varphi(x_k)-b)  \geq  1-\xi_k, & k=1,...,n \\
  & \hspace*{11ex} \xi_k  \geq  0, &  k=1,...,n.\nonumber
\end{eqnarray*}
y el clasificador SVM toma la siguiente forma
\begin{equation}
y(x)=sign\left(\sum_{k=1}^n\alpha_ky_kK(x,x_k)+b\right)
\end{equation}

Diferentes Kernels han sido usados en la literatura pero los más populares son: Linear, Polinomial and Función de Base Radial (\textit{\textbf{Radial Basis Functions}}, o \textit{\textbf{RBF}}). Definidas de la siguiente manera:
\begin{eqnarray*}
K_{linear}(x_k,x_\ell) & = & x_k^Tx_\ell ,\\
K_{polinomial}(x_k,x_\ell) & = & (1+x_k^Tx_\ell)^d ,\nonumber \\
K_{RBF}(x_k,x_\ell) & = & e^{-\frac{\|x_k-x_\ell\|^2}{\sigma^2}}
\end{eqnarray*}


\subsubsection{SVM Multiclase}
Obervar que SVM es un clasificador binario. Hay dos filosofías básicas para resolver la clasificación de datos en más de dos clases:
\begin{itemize}
\item una clase contra el resto (\textit{\textbf{one-versus-all}}),
\item entre cada par (\textit{\textbf{one-versus-one}}); se construyen $\frac{k(k-1)}{2}$ modelos para $k$ clases.
\end{itemize}



\subsubsection{Grilla de Valores}

En este trabajo los mejores resultados se consiguieron con el Kernel RBF ($K_{RBF}$), el cuál posee un parámetro $\sigma$ que debe ser elegido convenientemente para cada conjunto de datos. También el parámetro $C$ debe ser escogido pues, como se indicó en la sección \ref{svm_lineal}, es el que controla la compensación entre los errores de entrenamiento y los márgenes. Al ser dos parámetros, se puede generar una grilla de valores he ir probando hasta encontrar dónde se llega al máximo dentro ella. Un ejemplo de este procedimiento puede verse en la figura \ref{fig:grid}, ignorar por el momento los resultados numéricos.
\begin{figure}[!htbp]
    \centering
    \vspace*{-0.1cm}
    \hspace*{2.5cm}
    \includegraphics[scale=1]{imagen/grid.pdf}
    \vspace*{-0.1cm}
    \caption{Grilla de valores de $C$ y $\sigma$, con el objetivo de encontrar los óptimos}
    \label{fig:grid}
\end{figure}


\subsubsection{LibSVM}

Se utilizó una implementación disponible que puede encontrarse en \cite{libsvm}.