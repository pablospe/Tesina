\section{Resultados}

En esta sección se indicará sobre qué conjuntos de datos se ha trabajado y la forma de evaluación. También se compararán los diferentes métodos descripto a lo largo del presente trabajo.

\subsection{Base de datos}

La base de datos utilizada se encuentra disponible en \cite{LaViola}. Dicha base de datos fue subdividida en dos partes: base de datos de dígitos y base de datos de letras. La primera contiene~$1100$ muestras de~$11$ personas distintas que escribieron cada número~$10$ veces. Y la base de datos de letras contiene cerca de~$3600$ muestras. En la figura \ref{fig:db_digitos} se muestran~$10$ ejemplos normalizados a $[0,1]$ de la base de datos de dígitos.
    \begin{figure}[!htbp]
    \centering
    \vspace*{-0.6cm}
    \includegraphics[scale=0.65]{imagen/db_digitos.pdf}
    \vspace*{-0.6cm}
    \caption{Ejemplos de la dase de datos de dígitos}
    \label{fig:db_digitos}
    \end{figure}

En caso de que especifique lo contrario, las gráficas que se muestran a continuación corresponden a la base de datos de dígitos, suspendiendo el análisis de los resultados para la base de datos de letras hasta la sección \ref{sec:features_generales}.

\subsection{Validación}

La validación cruzada (o \textit{\textbf{cross-validation}}), es la práctica de partir una base de datos en subconjuntos de tal forma que el entrenamiento es realizado en algunos de ellos (\textit{\textbf{training}}), mientras los otros subconjuntos son retenidos para su uso posterior en la confirmación y validación del análisis inicial (\textit{\textbf{testing}}).

En \textit{\textbf{k-fold} cross-validation}, la base de datos es dividida en $k$ particiones. De las $k$ particiones, solo una se mantiene como datos de validación para testing, y las restante $k-1$ se utilizan para el entrenamiento. El proceso es repetido $k$ veces, de tal modo que cada una de las $k$ particiones es usada exactamente una vez para testing. Los $k$ resultados pueden ser promediados para producir una sola estimación. En este trabajo se ha utilizado 10-fold cross-validation.


\subsection{Entendiendo las gráficas}

A lo largo de esta sección se incluirán gráficas similares a la figura \ref{fig:graficas}.
    \begin{figure}[!htbp]
    \centering
%     \vspace*{-0.3cm}
    \includegraphics[scale=0.7]{imagen/plot/least_square_L_preproceso_1.pdf}
%     \vspace*{-0.5cm}
    \caption{Ejemplos de gráfica.}
    \label{fig:graficas}
    \end{figure}

En el título de las gráficas se indica la combinación de métodos y opciones usadas. A continuación se detallan los posibles valores:
\begin{itemize}
 \item  \textbf{Polinomio:} Legendre, Chebyshev o Legendre-Sobolev;
 \item  \textbf{Método:} Momentos o Mínimo Cuadrados (mediante pseudo-inversa, \textit{LeastSquare});
 \item  \textbf{Parametrización:} por tiempo o por longitud de arco (\textit{arc-length});
 \item  \textbf{Preproceso:} activado ($1$) o desactivado ($0$);
 \item  \textbf{libsvm:} los dos primeros parámetros son $C$ y $\gamma$, explicados en \ref{sec:grid}. Estos son pasado a la biblioteca \textit{libsvm}. Los demás parámetros son opciones internas de libsvm, que han elegido convenientemente y dejado fijas en todas las pruebas.
\end{itemize}

El eje $X$ de las gráficas representan el grado $d$ (\textit{degree}) de los polinomios elegidos, donde $d~\in~[3,20]$. Recordar que los \textit{features} son los vectores $({\alpha}_{0}, \dots ,{\alpha}_{d}, {\beta}_{0}, \dots ,{\beta}_{d})$ que tiene dimensión $2\,(d+1)$.

El eje $Y$ representa la \textit{\textbf{precisión}} alcanzada por el método, porcentaje de reconocimiento correcto, en el rango $[91,100]$.

Por último, la misma leyenda es utilizadas en todas las gráficas. Allí se indica el color de los métodos de \textit{clasificación} usados: $k$-NN, con alguna de sus distancias (Euclidiana, Cityblock o Mahalanobis), o LibSVM.


\subsection{Momentos vs Pseudo-inversa}
\subsection{Preprocesar o No Preprocesar}
\label{prepoceso}

Como se indicó en la sección \ref{sec:conceptos_preproceso}, antes del cálculo de los features se suelen usar~$3$ filtros como preproceso. Estos filtros son: 1. suavizado, 2. resampling, y 3. resizing (no necesariamente en ese orden). En lo siguiente se desea examinar la posibilidad de eliminar estos pasos en pos de eficiencia.

% El algoritmo de cálculo de momento puede ser implementado (como se vio en la sec) de manera tal que puedan ser calculados a medida que los datos se vayan introduciendo.

%Evitar utilizar estos filtros permitiría el cálculo de los momentos a medida que se van ingresando los datos, como se ha indicado en la sección \ref{sec:calculo_numerico_momentos}; lo cuál permitiría un aumento considerable de eficiencia en comparación con el método de la pseudo-inversa, el cual necesita esperar a que el usuario termine


\subsubsection{Evitando suavizado}
Recordar que los features son los coeficientes de polinomios, obtenidos por aproximación de mínimos cuadrados, lo cual genera una curva suave. El objetivo del suavizado (la eliminación del ruido presente al principio y final de cada trazo) es alcanzado sin necesidad de realizar un filtrado. Entonces, no es necesario este paso.

\subsubsection{Evitando resampling}
Al cambiar la representación de los trazos, de secuencia de puntos a curvas continuas, no hay necesidad de espaciarlos uniformemente o reducir/aumentar la cantidad de puntos en los trazos (tarea del filtro en cuestión). Si la velocidad en la que es escrito un símbolo afecta el reconocimiento, se puede optar por la reparametrización por longitud de arco, sección \ref{sec:arc-length}. Por lo tanto, tampoco es necesario el resampling.

\subsubsection{Evitando resizing}
Se ha demostrado que los features obtenidos son invariantes a escala, sección \ref{Invariante_escala}. Por lo que también es innecesario aplicar este filtro.

\subsubsection{Evitando traslación}
Un filtro no mencionado es aplicar traslación, que se ocupa de trasladar el trazo de manera tal que el mínimo de cada eje sea $0$. Se ha expresado que los features no son invariantes a traslación. Aquí se muestra por experimentación, que las traslaciones no provocan una pérdida significativa en la precisión de reconocimiento. Esto permitiría tomar dos caminos posibles al implementar un sistema de reconocimiento: preprocesar o no hacerlo. Observar que a pesar de que el preproceso se haya reducido a una simple traslación, el mínimo de cada eje es conocido recién cuando el trazo es finalizado, no permitiendo así el cálculo de los momentos a medida que se escribe.

Esta decisión de diseño puede ser tomada para el caso en que se requiera extrema eficiencia, como el caso de los dispositivos móviles, evitando preprocesar a costa de perder precisión. Entonces, se tiene que evaluar en qué escenario se desea utilizar el sistema de reconocimiento, y elegir el balance justo entre eficiencia y precisión según sea el caso. Una comparación sobre la precisión de reconocimiento puede verse en~\ref{fig:prep_vs_no_prep}, donde se muestra en \textbf{(a)} una pérdida de precisión del $2\%$ (considerando los mejores resultados) al no usar preproceso, con respecto a~\textbf{(b)} que sí utiliza preproceso.
% \vspace*{-0.6cm}
\begin{figure}[!htbp]
  \centering
  \advance\leftskip-2.8cm
  \advance\rightskip-2.8cm
  \subfloat[Sin prepoceso]{\label{fig:prep}\includegraphics[scale=0.46,keepaspectratio=true]{imagen/plot/moments_L_preproceso_0.pdf}}
  \subfloat[Con prepoceso]{\label{fig:no_prep}\includegraphics[scale=0.46,keepaspectratio=true]{imagen/plot/moments_L_preproceso_1.pdf}}
  \caption{Momentos con polinomios de Legendre}
  \label{fig:prep_vs_no_prep}
\end{figure}


% \begin{figure}[h!]
%  \centering
%  \advance\leftskip-2.8cm
%  \advance\rightskip-2.8cm
%  \includegraphics[scale=0.46,keepaspectratio=true]{imagen/plot/moments_L_preproceso_0.pdf}
% \hspace*{-0.4cm}
% \includegraphics[scale=0.46,keepaspectratio=true]{imagen/plot/moments_L_preproceso_1.pdf}
%  \caption{Sin preproceso (izq.), y con preproceso (der.). Método: Momentos con polinomios de Legendre}
%  \label{fig:prep_vs_no_prep}
% \end{figure}



%  \includegraphics[scale=0.458,keepaspectratio=true]{imagen/plot/moments_L_preproceso_1.pdf}
%  \includegraphics[scale=0.46,keepaspectratio=true]{imagen/plot/moments_L_arc_preproceso_1.pdf}


\subsection{Parametrización por tiempo vs Parametrización por longitud de arco}
\subsection{Representación a elegir}
\subsubsection{Legendre, Chebyshev o Legendre-Sobolev}
\subsubsection{Elección del grado de los polinomios}

\subsection{Momentos como features}


\subsection{Features de propósitos general}
\label{sec:features_generales}
Observar que los features no fueron diseñados especialmente para símbolos particulares, como ser los dígitos o las letras. Con la gráfica \ref{fig:letras_LS}, se pretende mostrar que los features obtenidos no dependen del símbolo a ser reconocido, pues se aplicó el mismo algoritmo (sin modificaciones) a la base de datos de letras, y se consiguieron también buenos resultados. % (casi $96\%$ con los polinomios de grado $10$).
Entonces, los features se pueden considerar de propósitos general pudiendo ser utilizados en diferentes situaciones, y no sólo con los dígitos.

En la gráfica recién mencionada, se muestran los mejores resultados en cuanto a precisión, obtenidos con los polinomios de Legendre-Sobolev de grado $9$ con poco más de $96\,\%$ de reconocimiento correcto para la base de dato de letras.
%; y en cuanto a \textbf{eficiencia}, obtenidos con momentos sin preprocesar los datos alcanzando un máximo de $9??\,\%$. Este último es el método más eficiente por las consideraciones hechas en el apartado \ref{prepoceso}.
    \begin{figure}[!htbp]
    \centering
    \vspace*{-0.2cm}
    \includegraphics[scale=0.65]{imagen/plot/least_square_LS_letras_preproceso_1.pdf}
    \vspace*{-0.2cm}
    \caption{Mejores resultados para la base de dato de letras.}
    \label{fig:letras_LS}
    \end{figure}

\subsection{Mejor performance}
\subsection{Mejor precisión}
